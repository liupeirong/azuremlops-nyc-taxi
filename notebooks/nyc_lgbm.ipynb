{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "from azureml.core import Workspace, Datastore, Dataset, Experiment, Environment\n",
    "from azureml.core.model import Model, InferenceConfig\n",
    "from azureml.core.webservice import AksWebservice, Webservice\n",
    "from azureml.core.compute import AksCompute\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_PATH = '../code'\n",
    "DATA_PATH = '../sample_data'\n",
    "DOWNLOAD_PATH = '../download'\n",
    "AML_UTIL_PATH = '../ml_service'\n",
    "sys.path.append(os.path.join(os.getcwd(), CODE_PATH))\n",
    "sys.path.append(os.path.join(os.getcwd(), AML_UTIL_PATH))\n",
    "import utils\n",
    "import consts\n",
    "import train\n",
    "import aml_utils as amlutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed when we run in a Jupyter notebook and the external files are changed\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "importlib.reload(consts)\n",
    "importlib.reload(train)\n",
    "importlib.reload(amlutils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "experiment = Experiment(ws, consts.experiment_name)\n",
    "compute_target_name = os.environ['AML_COMPUTE']\n",
    "inference_target_name = os.environ['AML_INFERENCE_COMPUTE']"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read raw data and prepare for training\n",
    "\n",
    "Read the raw data in the sample_data_folder, or get sample data from Azure Open DataSet as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.opendatasets import NycTlcGreen\n",
    "\n",
    "dfjan = pd.DataFrame([])\n",
    "start = datetime.strptime(\"1/1/2015\",\"%m/%d/%Y\")\n",
    "end = datetime.strptime(\"1/31/2015\",\"%m/%d/%Y\")\n",
    "\n",
    "for sample_month in range(1):\n",
    "    temp_df_green = NycTlcGreen(start + relativedelta(months=sample_month), end + relativedelta(months=sample_month)) \\\n",
    "        .to_pandas_dataframe()\n",
    "    dfjan = dfjan.append(temp_df_green.sample(1000))\n",
    "\n",
    "dfjan.to_csv(os.path.join(DATA_PATH, 'raw/201501.csv'))    \n",
    "dfjan.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder = os.path.join(DATA_PATH, 'raw')\n",
    "dfraw = utils.read_raw_data(raw_data_folder)\n",
    "dfraw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_folder = os.path.join(DATA_PATH, 'train')\n",
    "dftrain = utils.process_raw_data(dfraw)\n",
    "dftrain.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.duration.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AML datastore if not already exists\n",
    "datastore_name = os.environ['AML_DATASTORE']\n",
    "try:\n",
    "    datastore = Datastore.get(ws, datastore_name=datastore_name)\n",
    "    print('datastore {} exists'.format(datastore_name))\n",
    "except Exception:\n",
    "    print('create datastore {}'.format(datastore_name))\n",
    "    container_name = os.environ[\"BLOB_CONTAINER\"]\n",
    "    account_name = os.environ[\"BLOB_ACCOUNTNAME\"]\n",
    "    account_key = os.environ[\"BLOB_ACCOUNT_KEY\"]\n",
    "\n",
    "    datastore = Datastore.register_azure_blob_container(\n",
    "        workspace=ws, \n",
    "        datastore_name=datastore_name, \n",
    "        container_name=container_name, \n",
    "        account_name=account_name,\n",
    "        account_key=account_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Upload prepared data so that it can be accessed when training remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.write_train_data(dftrain, train_data_folder, '201501.csv')\n",
    "datastore.upload_files(files=[os.path.join(train_data_folder, '201501.csv')], target_path='train', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train locally, only use Azure ML for logging and uploading model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_run = experiment.start_logging()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train.split_data(dftrain)\n",
    "model, rmse, mape = train.train_model(x_train, x_test, y_train, y_test)\n",
    "\n",
    "local_run.log('rmse', rmse)\n",
    "local_run.log('mape', mape)\n",
    "print(\"rmse:{0}, mape:{1}\".format(rmse, mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test)\n",
    "actual_vs_predicted = y_test.to_frame()\n",
    "actual_vs_predicted['predicted'] = y_predict\n",
    "actual_vs_predicted.sort_index().plot(figsize=(20, 5), rot=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('outputs', exist_ok=True)\n",
    "model_file = os.path.join('outputs', consts.model_name)\n",
    "print('writing model file to {}'.format(model_file))\n",
    "joblib.dump(value=model, filename=model_file)\n",
    "local_run.upload_file(name=consts.model_name, path_or_stream=model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configure training environment and dataset to make training repeatable locally and remotely\n",
    "* Package dependencies for training are defined in conda dependency file train_env.yml.\n",
    "* Training data needs to be stored in blob storage for both local and remote compute can access it. The dataset doesn't have to be registered in order to mount or download to the compute resource.\n",
    "* Compared to the above way of training locally, the following approach unifies local and remote training with the same code. However, it doesn't seem to work on a Windows local machine. Use a Notebook VM instead for local training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "# if an existing env of specified name exists, we'll use it, otherwise create a new one\n",
    "use_existing_env = False\n",
    "# if a new env needs to be created, enable docker.  Azure ML can use either docker for training, or just conda environments\n",
    "enable_docker = False\n",
    "# if a new env is created, also register it\n",
    "register_new_env = False\n",
    "\n",
    "if use_existing_env:\n",
    "    found_existing_env = False\n",
    "    try:\n",
    "        training_env = Environment.get(ws, name=consts.train_environment_name)\n",
    "        found_existing_env = True\n",
    "        print('found existing env {}'.format(training_env.name))\n",
    "    except Exception:\n",
    "        print('didnot find existing env {}'.format(consts.train_environment_name))\n",
    "\n",
    "# if we don't want to use existing env, or we didn't find existing env, create a new one\n",
    "if (use_existing_env and found_existing_env):\n",
    "    if training_env.docker.enabled != enable_docker:\n",
    "        print('existing env has different docker settings than what you specified!')\n",
    "else:\n",
    "    print('create new env {}'.format(consts.train_environment_name))\n",
    "    training_env = Environment.from_conda_specification(name = consts.train_environment_name,\n",
    "                                                        file_path = os.path.join(CODE_PATH, 'train_env.yml'))\n",
    "    if enable_docker:\n",
    "        training_env.docker.enabled = True\n",
    "        training_env.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "        training_env.python.user_managed_dependencies = False\n",
    "    else:\n",
    "        training_env.python.user_managed_dependencies = True        \n",
    "    if register_new_env:\n",
    "        training_env.register(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If you train with ScriptRun, you need to create a training dataset so that it can be\n",
    "* downloaded for local training without Docker\n",
    "* mounted for local training with Docker or remote training\n",
    "\n",
    "It can be in-memory, doesn't have to be registered. TabularDataset has a method to_pandas_dataframes(), but it doesn't provide the same flexibility as pandas dataframe read_csv(). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train_dataset = Dataset.File.from_files(path=(datastore, 'train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If you train with Estimator, you can directly mount or download from datastore as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_local = True\n",
    "\n",
    "if train_local:\n",
    "    compute_target = 'local'\n",
    "    if enable_docker:\n",
    "        data_folder = datastore.path('train').as_mount()\n",
    "    else:\n",
    "        data_folder = datastore.path('train').as_download(DOWNLOAD_PATH)\n",
    "else:\n",
    "    compute_target = ws.compute_targets[compute_target_name]\n",
    "    data_folder = datastore.path('train').as_mount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "est = Estimator(\n",
    "        source_directory=CODE_PATH,\n",
    "        entry_script='train.py',\n",
    "        script_params={'--data_folder': data_folder},\n",
    "        compute_target=compute_target,\n",
    "        environment_definition=training_env)\n",
    "\n",
    "run = experiment.submit(config=est)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Register the model\n",
    "This is where we need to register a TabularDataset with the model in order to do data drift detection later. Create a TabularDataset pointing to the data used for training, and has the same schema as scoring input (which doesn't have dataframe index column!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_train_dataset=Dataset.Tabular.from_delimited_files(path=[(datastore, 'train')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run.register_model(\n",
    "    model_path=os.path.join('outputs', consts.model_name),\n",
    "    model_name=consts.model_name,\n",
    "    description='Lightgbm model for predicting taxi trip duration',\n",
    "    datasets=[(Dataset.Scenario.TRAINING, tabular_train_dataset)])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### download the model, make some predictions, and plot feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Model(ws, consts.model_name)\n",
    "#model.download(target_dir=DOWNLOAD_PATH)\n",
    "gbm = joblib.load(os.path.join(DOWNLOAD_PATH, consts.model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is an array of datapoints, each has an array of features\n",
    "input_sample = np.array([[1,1,1.00,-73.957909,40.670761,-73.952194,40.662312,8.15,1,17,5,1]]) \n",
    "# output is an array of predictions\n",
    "output_sample = gbm.predict(input_sample)\n",
    "output_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(gbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deploy the model as a web service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_env = Environment.from_conda_specification(\n",
    "    name = consts.inference_environment_name,\n",
    "    file_path = os.path.join(CODE_PATH, 'inference_env.yml'))\n",
    "inference_config = InferenceConfig(source_directory = CODE_PATH,\n",
    "                                   entry_script = 'score.py',\n",
    "                                   environment = inference_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aks_target = AksCompute(ws, inference_target_name)\n",
    "deployment_config = AksWebservice.deploy_configuration(\n",
    "    cpu_cores = 1, memory_gb = 2, collect_model_data=True, enable_app_insights=True)\n",
    "\n",
    "try: \n",
    "    service = Webservice(ws, consts.service_name)\n",
    "    print(\"Service {} exists, update it\".format(consts.service_name))\n",
    "    service.update(models=[model], inference_config=inference_config)\n",
    "except:\n",
    "    print('deploy a new service {}'.format(consts.service_name))\n",
    "    service = Model.deploy(ws, consts.service_name, [model], inference_config, deployment_config, aks_target)\n",
    "    service.wait_for_deployment(show_output = True)\n",
    "    print(service.state)\n",
    "    print(service.get_logs())\n",
    "\n",
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test against the deployed service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "if service.auth_enabled:\n",
    "    headers['Authorization'] = 'Bearer '+service.get_keys()[0]\n",
    "elif service.token_auth_enabled:\n",
    "    headers['Authorization'] = 'Bearer '+service.get_token()[0]\n",
    "\n",
    "print(headers)\n",
    "\n",
    "test_sample = json.dumps({'data': [\n",
    "    [1,1,1.00,-73.957909,40.670761,-73.952194,40.662312,8.15,1,17,5,1]\n",
    "]})\n",
    "#test_sample = json.dumps({'data': score_df.values.tolist()})\n",
    "\n",
    "response = requests.post(service.scoring_uri, data=test_sample, headers=headers)\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data drift\n",
    "* Create a DataDriftDetector\n",
    "* Inference on a dataset different from that was used for training\n",
    "* Run detection and see the distance between the training and inference datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_emails = [os.environ['ALERT_EMAIL']]\n",
    "\n",
    "#model = Model(ws, consts.model_name)\n",
    "feature_list = list(tabular_train_dataset.to_pandas_dataframe().drop(columns=['duration']).columns)\n",
    "monitor = amlutils.create_data_drift_detector_for_model(\n",
    "        ws, model, consts.service_name, compute_target_name,\n",
    "        feature_list, alert_emails, 0.1)\n",
    "\n",
    "monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training was done on Jan 2015 data, influence on July 2015 data to see if there's data drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.opendatasets import NycTlcGreen\n",
    "\n",
    "dfjuly = pd.DataFrame([])\n",
    "start = datetime.strptime(\"7/1/2015\",\"%m/%d/%Y\")\n",
    "end = datetime.strptime(\"7/31/2015\",\"%m/%d/%Y\")\n",
    "\n",
    "for sample_month in range(1):\n",
    "    temp_df_green = NycTlcGreen(start + relativedelta(months=sample_month), end + relativedelta(months=sample_month)) \\\n",
    "        .to_pandas_dataframe()\n",
    "    dfjuly = dfjuly.append(temp_df_green.sample(1000))\n",
    "\n",
    "dfjuly.to_csv(os.path.join(DATA_PATH, 'score/201507.csv'))    \n",
    "dfjuly.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Raw inference data must be processed the same way as training data was processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfjuly = utils.read_raw_data(os.path.join(DATA_PATH, 'score'))\n",
    "df_score = utils.process_raw_data(dfjuly)\n",
    "x_df_score = df_score.drop(columns = 'duration')\n",
    "x_df_score.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inference on the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "service = Webservice(ws, consts.service_name)\n",
    "data = json.dumps({'data': x_df_score.values.tolist()})\n",
    "\n",
    "data_encoded = bytes(data, encoding='utf8')\n",
    "prediction = service.run(input_data=data_encoded)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{ Wait 10 minutes or so for the inference data to be collected to Azure blob storage. Check the _modeldata_ container of the storage account associated with your Azure ML workspace to ensure inference data has been collected. }\n",
    "### Run data drift detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.utcnow()\n",
    "drift_run = monitor.run(now, [consts.service_name], feature_list=feature_list, compute_target=compute_target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a while for the run to complete and data to show up.\n",
    "child_run = list(drift_run.get_children())[0]\n",
    "child_run.get_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('azureml-py36': conda)",
   "language": "python",
   "name": "python36964bitazuremlpy36condac6eb40e913fd4163b54b46531f591a54"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}